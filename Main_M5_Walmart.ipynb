{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M5 Forecasting - Model Training and Inference\n",
        "\n",
        "## Overview\n",
        "This notebook implements the training pipeline using **Polars** for high-performance data engineering and **LightGBM** for gradient boosting. The strategy involves:\n",
        "\n",
        "1.  **Data Processing:** Converting raw CSVs to Parquet with strict type casting (Int16/Float32) to minimize memory usage.\n",
        "2.  **Feature Engineering:** Generating rolling windows and lags dynamically per store to avoid OOM errors.\n",
        "3.  **Modeling:** Training 10 separate store-level models using **Tweedie loss** to handle the zero-inflated nature of retail sales.\n",
        "4.  **Inference:** A two-stage prediction process (Validation days 1914-1941 and Evaluation days 1942-1969)."
      ],
      "metadata": {
        "id": "wYnlVFs10VDD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyXNb3zN0XH_",
        "outputId": "5e463001-c7d1-4fe4-bc59-7fbc2e91dc36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import polars as pl\n",
        "import gc\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "\n",
        "# I assume that you already downloaded the files needed as showed in the EDA.\n",
        "\n",
        "os.makedirs('./data', exist_ok=True)\n",
        "\n",
        "pl.scan_csv('calendar.csv').sink_parquet('./data/calendar.parquet')\n",
        "pl.scan_csv('sell_prices.csv').sink_parquet('./data/sell_prices.parquet')\n",
        "pl.scan_csv('sales_train_evaluation.csv').sink_parquet('./data/sales_train_evaluation.parquet')\n",
        "\n",
        "path_calendar = './data/calendar.parquet'\n",
        "path_prices = './data/sell_prices.parquet'\n",
        "path_sales = './data/sales_train_evaluation.parquet'\n",
        "path_base_output = './data/train_features'\n",
        "\n",
        "PREDICT_DAYS = 28\n",
        "ROLLING_WINDOWS = [7, 14, 30, 60, 140]\n",
        "STORES_LIST = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
        "WEEKS_LAGS = [1, 2, 3, 4]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'tweedie',\n",
        "    'tweedie_variance_power': 1.1,\n",
        "    'metric': 'rmse',\n",
        "    'subsample': 0.5,\n",
        "    'subsample_freq': 1,\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 2**11 - 1,\n",
        "    'min_data_in_leaf': 2**12 - 1,\n",
        "    'feature_fraction': 0.5,\n",
        "    'max_bin': 100,\n",
        "    'boost_from_average': False,\n",
        "    'verbose': -1,\n",
        "    'seed': 69,\n",
        "    'n_jobs': -1,\n",
        "    'force_col_wise': True\n",
        "}\n",
        "\n",
        "best_iters = {\n",
        "    'CA_1': 450, 'CA_2': 1000, 'CA_3': 650, 'CA_4': 350,\n",
        "    'TX_1': 150, 'TX_2': 90, 'TX_3': 90,\n",
        "    'WI_1': 550, 'WI_2': 650, 'WI_3': 200\n",
        "}"
      ],
      "metadata": {
        "id": "qNashXqS3EZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_store_week(store_name, min_lag, train_limit_day):\n",
        "\n",
        "    index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
        "    CURRENT_LAGS = [min_lag + (i * 7) for i in range(7)]\n",
        "    ROLLING_SHIFT = min_lag\n",
        "\n",
        "    train_df = (\n",
        "        pl.scan_parquet(path_sales)\n",
        "        .filter(pl.col(\"store_id\") == store_name)\n",
        "        .unpivot(index=index_columns, variable_name='d', value_name='sales')\n",
        "        .with_columns(\n",
        "            pl.col(\"d\").str.slice(2).cast(pl.Int16),\n",
        "            pl.col(\"sales\").cast(pl.Float32)\n",
        "        )\n",
        "        .filter(pl.col(\"d\") <= train_limit_day)\n",
        "    )\n",
        "\n",
        "    future_days_df = pl.select(\n",
        "        pl.int_range(\n",
        "            start=train_limit_day + 1,\n",
        "            end=train_limit_day + PREDICT_DAYS + 1,\n",
        "            dtype=pl.Int16,\n",
        "            eager=False\n",
        "        ).alias(\"d\")\n",
        "    ).lazy()\n",
        "\n",
        "    unique_items_df = train_df.select(index_columns).unique()\n",
        "    add_test = unique_items_df.join(future_days_df, how=\"cross\")\n",
        "\n",
        "    cols = train_df.collect_schema().names()\n",
        "    add_test = add_test.with_columns(pl.lit(None).cast(pl.Float32).alias(\"sales\")).select(cols)\n",
        "    train_df = pl.concat([train_df, add_test])\n",
        "\n",
        "    train_df = train_df.with_columns([pl.col(c).cast(pl.Categorical) for c in index_columns])\n",
        "\n",
        "    release_df = (\n",
        "        pl.scan_parquet(path_prices)\n",
        "        .filter(pl.col(\"store_id\") == store_name)\n",
        "        .with_columns([pl.col(\"store_id\").cast(pl.Categorical), pl.col(\"item_id\").cast(pl.Categorical)])\n",
        "        .group_by(['store_id', 'item_id'])\n",
        "        .agg(pl.col('wm_yr_wk').min().alias('release'))\n",
        "    )\n",
        "    train_df = train_df.join(release_df, on=['store_id', 'item_id'], how='left')\n",
        "\n",
        "    icols = ['date', 'd', 'wm_yr_wk', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
        "             'snap_CA', 'snap_TX', 'snap_WI', 'year', 'month', 'wday']\n",
        "\n",
        "    calendar = (\n",
        "        pl.scan_parquet(path_calendar)\n",
        "        .select(icols)\n",
        "        .with_columns(pl.col(\"d\").str.slice(2).cast(pl.Int16))\n",
        "        .with_columns([\n",
        "             pl.col('date').str.to_date().alias('dt_date'),\n",
        "             pl.col(['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']).cast(pl.Categorical),\n",
        "             pl.col(['snap_CA', 'snap_TX', 'snap_WI']).cast(pl.Int8),\n",
        "             pl.col('wm_yr_wk').cast(pl.Int16)\n",
        "        ])\n",
        "        .with_columns([\n",
        "            pl.col('dt_date').dt.day().cast(pl.Int8).alias('tm_d'),\n",
        "            pl.col('dt_date').dt.week().cast(pl.Int8).alias('tm_w'),\n",
        "            pl.col('month').cast(pl.Int8).alias('tm_m'),\n",
        "            (pl.col('year') - pl.col('year').min()).cast(pl.Int8).alias('tm_y'),\n",
        "            (pl.col('wday') - 1).cast(pl.Int8).alias('tm_dw'),\n",
        "            (pl.col('dt_date').dt.day() / 7).ceil().cast(pl.Int8).alias('tm_wm'),\n",
        "            (pl.col('tm_dw') >= 5).cast(pl.Int8).alias('tm_w_end')\n",
        "        ])\n",
        "        .drop(['date', 'dt_date', 'year', 'month', 'wday'])\n",
        "    )\n",
        "    train_df = train_df.join(calendar, on='d', how='left')\n",
        "    train_df = train_df.with_columns((pl.col('release') - pl.col('release').min()).cast(pl.Int16))\n",
        "\n",
        "    calendar_prices = pl.scan_parquet(path_calendar).select(['wm_yr_wk', 'month', 'year']).unique(subset=['wm_yr_wk'])\n",
        "    prices_df = (\n",
        "        pl.scan_parquet(path_prices)\n",
        "        .filter(pl.col(\"store_id\") == store_name)\n",
        "        .with_columns([\n",
        "            pl.col(\"store_id\").cast(pl.Categorical), pl.col(\"item_id\").cast(pl.Categorical), pl.col(\"wm_yr_wk\").cast(pl.Int16)\n",
        "        ])\n",
        "        .join(calendar_prices, on='wm_yr_wk', how='left')\n",
        "        .sort(['store_id', 'item_id', 'wm_yr_wk'])\n",
        "        .with_columns([\n",
        "            pl.col('sell_price').max().over(['store_id', 'item_id']).alias('price_max'),\n",
        "            pl.col('sell_price').min().over(['store_id', 'item_id']).alias('price_min'),\n",
        "            pl.col('sell_price').std().over(['store_id', 'item_id']).alias('price_std'),\n",
        "            pl.col('sell_price').mean().over(['store_id', 'item_id']).alias('price_mean'),\n",
        "            pl.col('sell_price').n_unique().over(['store_id', 'item_id']).alias('price_nunique'),\n",
        "            (pl.col('sell_price') / pl.col('sell_price').shift(1).over(['store_id', 'item_id'])).alias('price_momentum'),\n",
        "            pl.col('sell_price').mean().over(['store_id', 'item_id', 'month']).alias('price_mean_m'),\n",
        "            pl.col('sell_price').mean().over(['store_id', 'item_id', 'year']).alias('price_mean_y')\n",
        "        ])\n",
        "        .with_columns([\n",
        "            (pl.col('sell_price') / pl.col('price_max')).alias('price_norm'),\n",
        "            (pl.col('sell_price') / pl.col('price_mean_m')).alias('price_momentum_m'),\n",
        "            (pl.col('sell_price') / pl.col('price_mean_y')).alias('price_momentum_y')\n",
        "        ])\n",
        "        .drop(['month', 'year', 'price_mean_m', 'price_mean_y'])\n",
        "    )\n",
        "\n",
        "    train_df = train_df.join(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
        "\n",
        "    train_df = train_df.sort(['id', 'd'])\n",
        "\n",
        "    lag_exprs = [pl.col('sales').shift(l).over('id').cast(pl.Float32).alias(f'lag_{l}') for l in CURRENT_LAGS]\n",
        "    rolling_mean_exprs = [\n",
        "        (pl.col('sales').shift(ROLLING_SHIFT).rolling_mean(window_size=w, min_samples=1).over('id').cast(pl.Float32).alias(f'rolling_mean_{w}_shift{ROLLING_SHIFT}'))\n",
        "        for w in ROLLING_WINDOWS\n",
        "    ]\n",
        "    rolling_std_exprs = [\n",
        "        (pl.col('sales').shift(ROLLING_SHIFT).rolling_std(window_size=w, min_samples=1).over('id').cast(pl.Float32).alias(f'rolling_std_{w}_shift{ROLLING_SHIFT}'))\n",
        "        for w in ROLLING_WINDOWS\n",
        "    ]\n",
        "\n",
        "    target_encoding_groups = [['item_id'], ['dept_id'], ['cat_id'], ['item_id', 'dept_id'], ['store_id', 'cat_id'], ['store_id', 'dept_id']]\n",
        "    target_enc_exprs = []\n",
        "    for cols in target_encoding_groups:\n",
        "        name = '_'.join(cols)\n",
        "        target_enc_exprs.append(pl.col('sales').mean().over(cols).cast(pl.Float32).alias(f'enc_{name}_mean'))\n",
        "        target_enc_exprs.append(pl.col('sales').std().over(cols).cast(pl.Float32).alias(f'enc_{name}_std'))\n",
        "\n",
        "    output_file = f\"{path_base_output}_{store_name}_week{int(min_lag/7)}.parquet\"\n",
        "    train_df.with_columns(lag_exprs + rolling_mean_exprs + rolling_std_exprs + target_enc_exprs).sink_parquet(output_file)\n",
        "\n",
        "    del train_df, prices_df, calendar, release_df\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "MvjBB7dX3SVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_feature_generation(end_train_day):\n",
        "    print(f\"Generating features for end day: {end_train_day}\")\n",
        "    weeks_lags = [7, 14, 21, 28]\n",
        "    with pl.StringCache():\n",
        "        for store in STORES_LIST:\n",
        "            for lag in weeks_lags:\n",
        "                process_store_week(store, min_lag=lag, train_limit_day=end_train_day)"
      ],
      "metadata": {
        "id": "KinGB99w3eKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Strategy\n",
        "\n",
        "We iterate through each store and train utilizing a rolling window approach. Due to memory constraints, features are generated on-the-fly.\n",
        "*   **Objective:** `tweedie` (variance_power=1.1) captures the sparse distribution of daily sales.\n",
        "*   **Granularity:** Training is performed at the store level to capture local patterns while maintaining sufficient data density."
      ],
      "metadata": {
        "id": "hhgCSJzd30QM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We train a LightGBM model for each store, and create our submission for each store. We use the tweedie distribution, as we are dealing with sparse datasets. Most sales are zero, as we saw in the EDA."
      ],
      "metadata": {
        "id": "KSBWO7d733O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict(end_train_day, output_path, check_path):\n",
        "    print(f\"Train until {end_train_day}. Predict next 28 days\")\n",
        "\n",
        "    run_feature_generation(end_train_day)\n",
        "\n",
        "    os.makedirs(check_path, exist_ok=True)\n",
        "    remove_cols = ['id', 'd', 'sales', 'date', 'wm_yr_wk']\n",
        "    cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
        "                'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
        "                'snap_CA', 'snap_TX', 'snap_WI']\n",
        "\n",
        "    for store_name in STORES_LIST:\n",
        "        best_it = best_iters.get(store_name)\n",
        "\n",
        "        for week_num in [1, 2, 3, 4]:\n",
        "            ckpt_file = f\"{check_path}/pred_{store_name}_week{week_num}.parquet\"\n",
        "            if os.path.exists(ckpt_file):\n",
        "                print(f\"{store_name} week {week_num} checkpoint exists, skipping...\")\n",
        "                continue\n",
        "\n",
        "            file_path = f\"{path_base_output}_{store_name}_week{week_num}.parquet\"\n",
        "            print(f\"Training {store_name}, Week {week_num}\")\n",
        "\n",
        "            df = pl.read_parquet(file_path)\n",
        "            features = [c for c in df.columns if c not in remove_cols]\n",
        "            current_cat_feats = [c for c in features if c in cat_cols and c in df.columns]\n",
        "\n",
        "            if current_cat_feats:\n",
        "                df = df.with_columns([pl.col(c).to_physical() for c in current_cat_feats])\n",
        "\n",
        "            mask_train = (pl.col('d') <= end_train_day)\n",
        "\n",
        "            X_train = df.filter(mask_train).select(features).to_numpy()\n",
        "            y_train = df.filter(mask_train).select('sales').to_numpy().flatten()\n",
        "\n",
        "            train_set = lgb.Dataset(X_train, label=y_train, feature_name=features, categorical_feature=current_cat_feats)\n",
        "            model = lgb.train(params, train_set, num_boost_round=best_it)\n",
        "\n",
        "            start_d = end_train_day + (week_num - 1) * 7 + 1\n",
        "            end_d = end_train_day + week_num * 7\n",
        "\n",
        "            mask_test = (pl.col('d') >= start_d) & (pl.col('d') <= end_d)\n",
        "            X_test = df.filter(mask_test).select(features).to_numpy()\n",
        "            df_test_meta = df.filter(mask_test).select(['id', 'd']).to_pandas()\n",
        "\n",
        "            if len(X_test) > 0:\n",
        "                preds = model.predict(X_test)\n",
        "                df_test_meta['sales'] = preds\n",
        "                pl.from_pandas(df_test_meta).write_parquet(ckpt_file)\n",
        "\n",
        "            del df, X_train, y_train, X_test, train_set, model, df_test_meta\n",
        "            gc.collect()\n",
        "\n",
        "    print(\"Consolidating submissions...\")\n",
        "\n",
        "    checkpoint_files = glob.glob(f\"{check_path}/*.parquet\")\n",
        "\n",
        "    submission_long = pl.scan_parquet(checkpoint_files).with_columns(pl.col(\"id\").cast(pl.String)).collect().to_pandas()\n",
        "\n",
        "    submission_long['F_col'] = submission_long['d'].apply(lambda x: f\"F{x - end_train_day}\")\n",
        "\n",
        "    submission_final = submission_long.pivot(index='id', columns='F_col', values='sales').reset_index()\n",
        "    cols_order = ['id'] + [f\"F{i}\" for i in range(1, 29)]\n",
        "    submission_final = submission_final.reindex(columns=cols_order, fill_value=0)\n",
        "\n",
        "    if end_train_day == 1913:\n",
        "        submission_final['id'] = submission_final['id'].str.replace('_evaluation', '_validation')\n",
        "\n",
        "    submission_final.to_csv(output_path, index=False)\n",
        "    print(f\"Submission saved to {output_path}\")"
      ],
      "metadata": {
        "id": "A15k1xvF3jTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_predict(\n",
        "    end_train_day=1913,\n",
        "    output_path='./data/submission.csv',\n",
        "    check_path='./data/checkpoints_val'\n",
        ")\n",
        "\n",
        "train_and_predict(\n",
        "    end_train_day=1941,\n",
        "    output_path='./data/submission2.csv',\n",
        "    check_path='./data/checkpoints_eval'\n",
        ")\n",
        "\n",
        "df_val = pd.read_csv('./data/submission.csv')\n",
        "df_eval = pd.read_csv('./data/submission2.csv')\n",
        "submission_final = pd.concat([df_val, df_eval], ignore_index=True)\n",
        "\n",
        "print(f\"Total rows: {len(submission_final)} (Expected: 60980)\")\n",
        "submission_final.to_csv('./data/submission_final.csv', index=False)\n",
        "print(\"Submission completed succesfully\")"
      ],
      "metadata": {
        "id": "q6J1x3ea38pd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}